
import React from 'react';
import { Project, Skill, Certificacao, Certification, Experience, Academic } from './types';
import projeto1 from './img/projeto-1.png';
import projeto2 from './img/projeto-2.jpg';
import certificacaoDp900 from './img/certificacao.png';

export const SITE_CONFIG = {
  name: "Arthur Almeida",
  role: "Engenheiro de Dados Microsoft Azure & Databricks",
  email: "arthur@example.com",
  github: "https://github.com/ArthurrAlmeida",
  linkedin: "https://www.linkedin.com/in/arthurvalmeida/",
  medium: "https://medium.com/@devarthuralmeida",
  about: [
    "Seja bem-vindo ao meu portf√≥lio. Aqui voc√™ encontrar√° projetos focados na constru√ß√£o de arquiteturas robustas e escal√°veis.",
    "Mestrando em Intelig√™ncia Artificial, com MBA em Big Data Analytics e certifica√ß√£o Microsoft DP-900 (Azure Data Fundamentals), possuo uma forma√ß√£o s√≥lida que une teoria avan√ßada √† pr√°tica de mercado.",
    "Expertise em governan√ßa de dados, otimiza√ß√£o de performance e desenvolvimento de pipelines complexos para ambientes de nuvem.",
    "Atualmente atuo como Professor Universit√°rio na √°rea de tecnologia, conciliando a execu√ß√£o t√©cnica de projetos com a doc√™ncia."
  ],
};

export const ACADEMIC_INFO: Academic[] = [
  {
    id: "acad1",
    institution: "Universidade Federal da Para√≠ba (UFPB)",
    degree: "Mestrando em Intelig√™ncia Artificial",
    period: "Em andamento",
    description: "Pesquisa avan√ßada em modelos generativos e otimiza√ß√£o de fluxos de processamento de dados com IA."
  },
  {
    id: "acad2",
    institution: "Faculdade Metropolitana",
    degree: "MBA em Big Data Analytics",
    period: "Conclu√≠do",
    description: "Especializa√ß√£o em arquitetura de dados, an√°lise estat√≠stica e ferramentas de processamento em larga escala."
  },
  {
    id: "acad3",
    institution: "Faculdade Internacional da Para√≠ba",
    degree: "Bacharel em Ci√™ncia da Computa√ß√£o",
    period: "Conclu√≠do",
    description: "Base s√≥lida em algoritmos, engineering de software e fundamentos de computa√ß√£o em nuvem."
  }
];

export const EXPERIENCES: Experience[] = [
  {
    "id": "exp1",
    "company": "Faculdade Internacional da Para√≠ba (FPB)",
    "role": "Professor de Computa√ß√£o em Nuvem & Dados",
    "period": "Ago 2025 ‚Äì Presente",
    "description": [
      "Ministro disciplinas focadas em Modern Data Stack (Azure, Databricks, SQL), preparando alunos para demandas reais de arquitetura escal√°vel.",
      "Desenvolvi metodologia de ensino pr√≥pria com material did√°tico personalizado, traduzindo conceitos complexos de Big Data para acelerar o aprendizado t√©cnico.",
      "Lidero projetos acad√™micos que simulam ambientes reais de Data Lakehouse, aplicando metodologias √°geis (Scrum/Kanban).",
      "Capacito turmas no uso pr√°tico de tecnologias de nuvem e Modelagem de Dados (Star Schema/Snowflake), conectando a teoria acad√™mica √†s exig√™ncias do mercado."
    ],
    "technologies": ["Azure", "Databricks", "SQL", "Spark", "Scrum", "Modern Data Stack"]
  },
  {
    "id": "exp2",
    "company": "Liga Digital",
    "role": "Engenheiro de Dados (Foco em IA)",
    "location": "Jo√£o Pessoa, PB",
    "period": "Abr 2024 ‚Äì Jul 2024",
    "description": [
      "Desenvolvi e automatizei fluxos de ETL/ELT em Python (Pandas) para alimenta√ß√£o de modelos de Intelig√™ncia Artificial.",
      "Implementei REST APIs para integra√ß√£o e orquestra√ß√£o de dados, garantindo alta disponibilidade e reduzindo a lat√™ncia na comunica√ß√£o entre sistemas.",
      "Otimizei pipelines de dados brutos, aumentando a efici√™ncia do processamento e a confiabilidade das entregas para clientes internacionais.",
      "Atuei na manuten√ß√£o cont√≠nua de pipelines de IA e Data Quality, garantindo a estabilidade e a performance dos modelos em produ√ß√£o."
    ],
    "technologies": ["Python", "Pandas", "Azure Data Factory", "REST APIs", "Data Quality", "Git"]
  },
  {
    "id": "exp3",
    "company": "Grupo Energisa",
    "role": "Engenheiro de Dados",
    "location": "Jo√£o Pessoa, PB (H√≠brido)",
    "period": "Ago 2022 ‚Äì Ago 2023",
    "description": [
      "Atuei na engenharia de dados do Ciclo de Faturamento, integrando fontes cr√≠ticas financeiras e de mercado para suportar an√°lises de receita.",
      "Arquitetei solu√ß√µes End-to-End utilizando Azure Data Factory (ADF) e Oracle Integrator Data, reduzindo o tempo de disponibilidade da informa√ß√£o de dias para horas.",
      "Otimizei consultas complexas em Oracle PL/SQL, obtendo ganho de 98% de performance e redu√ß√£o de 40% nos custos de processamento em nuvem (FinOps).",
      "Implementei l√≥gicas de deduplica√ß√£o e tratamento de dados para o Data Warehouse, assegurando a confiabilidade dos dashboards financeiros."
    ],
    "technologies": ["Azure Data Factory", "Oracle PL/SQL", "Azure Cosmos DB", "Power BI", "FinOps", "Data Warehouse"]
  },
  {
    "id": "exp4",
    "company": "Job Space Creative",
    "role": "Desenvolvedor FullStack",
    "location": "Curitiba, PR (Remoto)",
    "period": "Fev 2022 ‚Äì Jul 2022",
    "description": [
      "Atuei no desenvolvimento e manuten√ß√£o de plataformas de e-commerce VTEX de alto tr√°fego para grandes marcas como Brit√¢nia e Philco.",
      "Implementei melhorias de front-end utilizando JavaScript, HTML e CSS, focando em responsividade e otimiza√ß√£o da experi√™ncia do usu√°rio (UX).",
      "Utilizei pipelines de CI/CD e versionamento com Git/GitHub para automatizar deploys e garantir a integridade do c√≥digo em produ√ß√£o.",
      "Colaborei em equipe utilizando metodologias √°geis para entrega cont√≠nua de funcionalidades e corre√ß√µes de bugs."
    ],
    "technologies": ["VTEX", "JavaScript", "CI/CD", "Git", "HTML5/CSS3"]
  }
];

export const PROJECTS: Project[] = [
  {
    id: "1",
    title: "Data Warehouse Moderno & Pipeline ETL no Azure",
    description: "Pipeline ETL completo (Bronze/Silver/Gold) usando ADF, Data Lake e SQL Database. Veja o c√≥digo e a documenta√ß√£o.",
    image: projeto1,
    tags: ["Azure Data Factory", "Azure SQL Database", "Azure Data Lake Gen2", "SQL"],
    link: "https://github.com/ArthurrAlmeida/Azure-data-warehouse-adventureworks",
    github: "https://github.com/ArthurrAlmeida/Azure-data-warehouse-adventureworks"
  },
  {
    id: "2",
    title: "API de Futebol em Tempo Real com NestJS",
    description: "Coleta de dados esportivos em tempo real com NestJS e TypeScript. Integra√ß√£o via API externa, persist√™ncia em PostgreSQL e documenta√ß√£o autom√°tica.",
    image: projeto2,
    tags: ["NestJS", "Stream Analytics", "Power BI", "PostgreSQL"],
    link: "https://github.com/ArthurrAlmeida/SistemaColetaDeDadosAPIFootball",
    github: "https://github.com/ArthurrAlmeida/SistemaColetaDeDadosAPIFootball"
  },
  /*{
    id: "3",
    title: "Framework de Governan√ßa de Dados",
    description: "Cria√ß√£o de automa√ß√µes para cat√°logo de dados e controle de acesso (RBAC) em ambientes multi-tenant de nuvem.",
    image: "./imagens/projeto3.jpg",
    tags: ["Unity Catalog", "Python", "Azure DevOps"],
    link: "#",
    github: "#"
  }*/
];

export const SKILLS: Skill[] = [
  // Core Stack
  { name: "Azure", icon: "‚òÅÔ∏è", category: "Core Stack" },
  { name: "Data Factory", icon: "üè≠", category: "Core Stack" },
  { name: "Databricks", icon: "üß±", category: "Core Stack" },
  { name: "Python", icon: "üêç", category: "Core Stack" },
  { name: "SQL", icon: "üìä", category: "Core Stack" },
  
  // Big Data & Cloud
  { name: "Spark", icon: "üî•", category: "Big Data & Cloud" },
  { name: "Synapse", icon: "üèõÔ∏è", category: "Big Data & Cloud" },
  { name: "Snowflake", icon: "‚ùÑÔ∏è", category: "Big Data & Cloud" },
  { name: "ADLS Gen2", icon: "üìÇ", category: "Big Data & Cloud" },
  { name: "Delta Lake", icon: "üåä", category: "Big Data & Cloud" },
  
  // Ops & Tools
  { name: "Git", icon: "üåø", category: "Ops & Tools" },
  { name: "CI/CD", icon: "üîÑ", category: "Ops & Tools" },
  { name: "Docker", icon: "üê≥", category: "Ops & Tools" },
  { name: "Airflow", icon: "üå¨Ô∏è", category: "Ops & Tools" },
  { name: "Power BI", icon: "üìà", category: "Ops & Tools" },
];

export const CERTIFICACOES: Certificacao[] = [
  {
    id: "dp-900",
    title: "Microsoft Certified: Azure Data Fundamentals",
    issuer: "Microsoft",
    date: "Jan 2026",
    image: certificacaoDp900, 
    link: "https://learn.microsoft.com/api/credentials/share/pt-br/ArthurAlmeida-9156/1D79FC0E01717B89?sharingId=A85CEBB003C9B809", 
    tags: ["Azure", "Data", "Cloud"]
  },

];

export const CERTIFICATIONS: Certification[] = [
  {
    id: "c3",
    title: "Docker",
    issuer: "Infinity BigData",
    date: "2026",
    icon: "üê≥",
    link: "#"
  },
  {
    id: "c4",
    title: "CI/CD & DevOps",
    issuer: "DuZeru",
    date: "2025",
    icon: "üîÑ",
    link: "#"
  },
  {
    id: "c5",
    title: "Databricks",
    issuer: "Infinity BigData",
    date: "2025",
    icon: "üß±",
    link: "#"
  },
  {
    id: "c6",
    title: "Databricks Data Enginner Associate",
    issuer: "Ramesh Retnasamy",
    date: "2025",
    icon: "üìú",
    link: "#"
  },
  {
    id: "c7",
    title: "Azure Data Factory",
    issuer: "Ramesh Retnasamy",
    date: "2025",
    icon: "üè≠",
    link: "#"
  },
  {
    id: "c8",
    title: "AWS Academy",
    issuer: "AWS",
    date: "2024",
    icon: "‚òÅÔ∏è",
    link: "#"
  },
  {
    id: "c9",
    title: "Apache Spark & PySpark",
    issuer: "Fernando Amaral",
    date: "2023",
    icon: "üî•",
    link: "#"
  },
  {
    id: "c2",
    title: "Banco de Dados",
    issuer: "Alura",
    date: "2022",
    icon: "üìä",
    link: "#"
  }
];
